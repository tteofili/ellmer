import logging

import numpy as np
import pandas as pd

from ellmer.post_hoc import certa_local_support, certa_triangles_method
from ellmer.post_hoc.utils import lattice, get_row

class LLMCertaExplainer(object):

    def __init__(self, lsource, rsource, data_augmentation: str = 'on_demand', token=False):
        '''
        Create the CERTA explainer
        :param lsource: the data source for "left" records
        :param rsource: the data source for "right" records
        :param data_augmentation: 'no' to avoid usage of DA at all, 'on_demand' to use it only when needed, 'always'
            to always use DA generated records even when the no. of found support records is sufficient.
        '''
        if data_augmentation in ['always', 'on_demand']:
            gen_left, gen_right = certa_local_support.generate_subsequences(lsource, rsource)
            self.lsource = pd.concat([lsource, gen_left])
            self.rsource = pd.concat([rsource, gen_right])
            if data_augmentation == 'always':
                self.use_all = True
            else:
                self.use_all = False
        else:
            self.lsource = lsource
            self.rsource = rsource
            self.use_all = False

    def explain(self, l_tuple, r_tuple, predict_fn, left=True, right=True, attr_length=-1,
                num_triangles: int = 100, lprefix='ltable_', rprefix='rtable_', verbose: bool = False,
                max_predict: int = -1, debug: bool = False, token: bool = False, two_step_token: bool = False,
                filter_features: list = None, support_samples: pd.DataFrame = None, llm=None, **kwargs):
        """
        Explain the prediction generated by an ER model via its prediction function predict_fn on a pair of records
         l_tuple and r_tuple.
        :param l_tuple: the "left" record
        :param r_tuple: the "right" record
        :param predict_fn: the ER model prediction function
        :param left: whether to use left open triangles
        :param right: whether to use right open triangles
        :param attr_length: the maximum length of sets of attributes to be considered for generating an explanation
        :param num_triangles: number of open triangles to be used to generate the explanation
        :param lprefix: the prefix of attributes from the "left" table
        :param rprefix: the prefix of attributes from the "right" table
        :param max_predict: the maximum number of predictions to be performed by the ER model to generate the requested
        number of open triangles
        :param debug: whether to produce lattice data for debugging
        :param two_step_token: whether to perform two step token explanation (attribute for feature selection, then token level)
        :param filter_features: let only a specific list of features be perturbed and checked for flipping
        :param support_samples: the support samples to use for the explanation
        :param llm: the black box llm to explain
        :return: saliency explanation, the probabilities of sufficiency, all the generated cf explanations,
                the open triangles, the support_samples used
        """

        prediction = certa_local_support.get_original_prediction(l_tuple, r_tuple, predict_fn)
        pc = np.argmax(prediction)
        if support_samples is None:
            if verbose:
                print(f'getting support')
            support_samples, gleft_df, gright_df = certa_local_support.support_predictions(l_tuple, r_tuple, self.lsource,
                                                                                     self.rsource,
                                                                                     predict_fn, lprefix, rprefix,
                                                                                     class_to_explain=pc, use_w=left,
                                                                                     use_q=right, use_all=self.use_all,
                                                                                     num_triangles=num_triangles,
                                                                                     max_predict=max_predict, llm=llm)
        else:
            _, gleft_df, gright_df = certa_local_support.expand_copies(lprefix, self.lsource, l_tuple, r_tuple, rprefix, self.rsource)

        if attr_length <= 0:
            attr_length = min(len(l_tuple) - 1, len(r_tuple) - 1)
        if len(support_samples) > 0:
            self.lsource = pd.concat([self.lsource, gright_df])
            self.rsource = pd.concat([self.rsource, gleft_df])
            extended_sources = [self.lsource.copy(), self.rsource.copy()]
            if verbose:
                print(f'traversing lattices')
            print(f'support size: {len(support_samples)}')
            print(support_samples)
            pns, pss, cf_ex, triangles = certa_triangles_method.explain_samples(support_samples, extended_sources, predict_fn,
                                                                                lprefix, rprefix, pc, attr_length,
                                                                                persist_predictions=debug, token=token,
                                                                                filter_features=filter_features, two_step_token=two_step_token,)
            cf_summary = certa_triangles_method.cf_summary(pss)
            saliency_df = pd.DataFrame(data=[pns.values()], columns=pns.keys())
            if len(cf_ex) > 0:
                cf_ex['attr_count'] = cf_ex.altered_attributes.astype(str) \
                    .str.split(',').str.len()
                cf_ex = cf_ex[cf_ex['altered_attributes'].isin([tuple(k.split('/')) for k in cf_summary.keys()])] \
                    .astype(str).drop_duplicates(subset=['copied_values', 'altered_attributes', 'dropped_values'])
            lattices = []
            if debug:
                # generate lattice debug data
                triangle_predictions = pd.read_csv('predictions.csv')
                gbo = triangle_predictions.groupby('triangle')
                triangle_ids = list(gbo.groups.keys())
                for i in np.arange(len(triangle_ids)):
                    try:
                        triangle = triangle_ids[i]
                        triangle_lattice = gbo.get_group(triangle)[['altered_attributes', 'match_score']]
                        triangle_lattice['altered_attributes'] = triangle_lattice['altered_attributes'].apply(lambda x: tuple(
                            x.replace("'", '').replace('(', '').replace(')', '').replace(',', '').split(' ')))
                        lattice_dict = dict(zip(triangle_lattice.altered_attributes, triangle_lattice.match_score)) # FIXME in token case, create more lattices / triangles out of triangle_lattice object
                        triangle_edges = triangle.split(' ')
                        if triangle[0].startswith('0'):
                            powerset = [set()] + [set(s) for s in lattice_dict.keys()] + [
                                set([c for c in saliency_df.columns if c[0] == 'l'])]
                            if pc == 0:
                                f = extended_sources[0][extended_sources[0]['ltable_id'] == int(triangle_edges[2].split('@')[1])].iloc[0]
                                s = extended_sources[0][extended_sources[0]['ltable_id'] == int(triangle_edges[0].split('@')[1])].iloc[0]
                            else:
                                f = extended_sources[0][extended_sources[0]['ltable_id'] == int(triangle_edges[0].split('@')[1])].iloc[0]
                                s = extended_sources[0][extended_sources[0]['ltable_id'] == int(triangle_edges[2].split('@')[1])].iloc[0]
                            p = extended_sources[1][extended_sources[1]['rtable_id'] == int(triangle_edges[1].split('@')[1])].iloc[0]
                            tl_tuple = s
                            tr_tuple = p
                        else:
                            powerset = [set()] + [set(s) for s in lattice_dict.keys()] + [
                                set([c for c in saliency_df.columns if c[0] == 'r'])]
                            if pc == 0:
                                f = extended_sources[1][extended_sources[1]['rtable_id'] == int(triangle_edges[2].split('@')[1])].iloc[0]
                                s = extended_sources[1][extended_sources[1]['rtable_id'] == int(triangle_edges[0].split('@')[1])].iloc[0]
                            else:
                                f = extended_sources[1][extended_sources[1]['rtable_id'] == int(triangle_edges[0].split('@')[1])].iloc[0]
                                s = extended_sources[1][extended_sources[1]['rtable_id'] == int(triangle_edges[2].split('@')[1])].iloc[0]
                            p = extended_sources[0][extended_sources[0]['ltable_id'] == int(triangle_edges[1].split('@')[1])].iloc[0]
                            tl_tuple = p
                            tr_tuple = s
                        tl_tuple.index = tl_tuple.index.str.lstrip("ltable_")
                        tr_tuple.index = tr_tuple.index.str.lstrip("rtable_")

                        top_lattice_prediction = certa_local_support.get_original_prediction(tl_tuple, tr_tuple, predict_fn)
                        if np.argmax(top_lattice_prediction) == pc:
                            top_lattice_prediction = certa_local_support.get_original_prediction(tr_tuple, tl_tuple, predict_fn)
                        rank = [prediction[1]] + list(lattice_dict.values()) + [top_lattice_prediction[1]]
                        if len(powerset) != len(rank):
                            print(f'skipping differing: {powerset}\n {rank}')
                            continue
                        triangle_df = pd.concat([p, f, s], axis=1).T
                        triangle_df['type'] = ['pivot', 'free', 'support']
                        lattice_predictions = gbo.get_group(triangle).drop(
                            ['Unnamed: 0', 'triangle', 'dropped_values', 'copied_values', 'nomatch_score'],
                            axis=1)

                        op = get_row(l_tuple, r_tuple).drop(['ltable_id', 'rtable_id'], axis=1)
                        op['altered_attributes'] = ''
                        op['match_score'] = prediction[1]

                        sp = get_row(tl_tuple, tr_tuple)
                        if 'ltable_id' in sp.columns:
                            sp = sp.drop(['ltable_id'], axis=1)
                        if 'rtable_id' in sp.columns:
                            sp = sp.drop(['rtable_id'], axis=1)
                        sp['altered_attributes'] = str(powerset[-1:][0])
                        sp['match_score'] = top_lattice_prediction[1]

                        lattice_predictions['altered_attributes'] = lattice_predictions['altered_attributes'].apply(
                            lambda x: tuple(
                                x.replace("'", '').replace('(', '').replace(')', '').replace(',', '').split(' ')))

                        lattice_predictions = pd.concat([sp, lattice_predictions, op], ignore_index=True)

                        lattice_predictions = lattice_predictions.sort_values(by="altered_attributes",
                                                                              key=lambda x: x.str.count(
                                                                                  '|'.join(['ltable_', 'rtable_'])),
                                                                              ascending=False)

                        latt = lattice(powerset, rank, triangle=lattice_predictions)
                        lattices.append(latt)
                    except:
                        pass

            return saliency_df, pss, cf_ex, triangles, lattices, support_samples
        else:
            logging.warning('no triangles found -> empty explanation')
            return pd.DataFrame(), pd.Series(), pd.DataFrame(), [], [], support_samples
